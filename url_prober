#!/usr/bin/python3
from urllib import parse
import sys
from collections import Counter
import argparse
import random

def process(iter):

    for line in iter:
        p = parse.urlparse(line.strip())
        if p.path.endswith(('.css', '.js', '.xml', '.ico')):
            continue
        yield p

def pages(data, domain):
    x = [p.scheme + '://' + p.netloc + p.path for p in data if p.netloc == domain]
    return x

def main ():
    data = list(process(sys.stdin.readlines()))
    top_pages = filter(lambda x: x[1] > 10,Counter([d.netloc+d.path for d in data]).most_common(30))
    print ("== Top 30 pages of %i" % len(data))
    for p in top_pages:
        print ("%s %i hits, %.5f%%" % (p[0], p[1], float(p[1])*100/len(data)))
    top_domains = filter(lambda x: x[1] > 20, Counter([d.netloc for d in data]).most_common(20))
    uniq_domains = len(set([d.netloc for d in data]))
    print( "== Top 30 of %s domains ==" % uniq_domains)

    for d in top_domains:
        print ("%s (%i hits, %.2f %%)" % (d[0], d[1], float(d[1]) / len(data) * 100))
        pag = filter(lambda x: x[1] > 0, Counter(pages(data, d[0])).most_common(10))
        for p in pag: 
            print ( '\t%s %i' % (p[0], p[1]))
#            print ("top")
#            pprint.pprint(filter(lambda x:x[1]>2,p))
#            print ("random")
#            pprint.pprint([random.choice(list(p)) for x in range(0,10)])

if __name__ == '__main__':
    main()

